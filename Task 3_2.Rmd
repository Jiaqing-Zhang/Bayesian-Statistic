---
title: "Task 3_2"
author: "Jiaqing Zhang"
date: "3/5/2019"
output: html_document
---
###1.4 Conditioning on the Data
The posterior median of the $R^2$ is about the same as the prior median for the $R^2$ we specified, which is 0.5.
```{r}
library(rstanarm)
wages<-readRDS("/Users/jiaqingzhang/Downloads/wages1.rds")
##do some transformation to recenter the predictors:
wages$age.1<-wages$AGEP-mean(wages$AGEP)
wages$WKHP.1<-wages$WKHP-mean(wages$WKHP)
wages$SCHL.1<-as.integer(wages$SCHL)-mean(wages$SCHL)
post<-stan_lm(log(WAGP)~AGEP+as.integer(SEX)+WKHP+as.integer(SCHL), data=wages, prior=R2(0.5, what="median"))
print(post, digits = 6)

```

###1.5 Average Wages
The probability that the posterior predictive distribution of the average wage is greater than the average observed value of WAGP is 0.536. However, the differences between the posterior predictive distribution of the average wage and the average observed value of WAGP are very small. Most of the difference are within 0.001. 
```{r}
post_pred<-posterior_predict(post, draws=250)
post_mean<-apply(post_pred, MARGIN = 1, FUN = mean)
length(post_mean)
summary(post_mean)
##average of the observed wages (transforming using log)
(mean_ob<-mean(log(wages$WAGP)))

sum(post_mean>mean_ob)/length(post_mean)
```

###2.1 Prior Predictive Distribution
```{r}
rstan::expose_stan_functions("bipoisson_PPD_rng.stan")
out<-bipoisson_PPD_rng(S=10000, rate_1 = 1/14 , rate_2 = 1/7 , rate_3 = 1/9 )
```


###2.2 Posterior Kernel
```{r}
rstan::expose_stan_functions("bipoisson_kernel.stan")
```

###3.1 Drawing from a Posterior Distribution
```{r}
library(dplyr)
users <- readr::read_csv("users.csv")
colnames(users)[1] <- "user_id"
names(users)
head(users, 6)

tweets <- readr::read_csv("tweets.csv")
names(tweets)
head(tweets)


dataset.3<-inner_join(users, tweets, by="user_id")  ##vertical type of data, one user-id may have several observations (lines).
dim(dataset.3)

dataset.3$Clinton <- as.integer(grepl("Clinton", dataset.3$text, ignore.case = TRUE) ) ##create a new variable that have tweets reference Clinton
dataset.3$Trump <- as.integer(grepl("Trump", dataset.3$text, ignore.case = TRUE) ) ##create a new variable that have tweets reference Trump

dataset.4<-dataset.3[, c(1, 4, 5, 12:14, 17:21, 30:31)]
dim(dataset.4)
dataset.4$date_1<-as.Date(dataset.4$created_str,'%Y/%m/%d')

##convert the dataset so that the new dataset will include the time period of the account and the number of times mentioning Clinton. All other predictors will be included as well.

 df.1<-group_by(dataset.4[complete.cases(dataset.4$date_1),], user_id) %>%
summarize(days = max(date_1)-min(date_1) +1, na.rm = TRUE)
 df.2<-group_by(dataset.4, user_id) %>%
summarize(number_tweets=length(user_id), na.rm = TRUE)
 df.3<-group_by(dataset.4, user_id) %>%
summarize(number_clinton=sum(Clinton), na.rm = TRUE)
 df.4<-group_by(dataset.4, user_id) %>%
summarize(number_trump=sum(Trump), na.rm = TRUE)
 df.5<-unique(dataset.4[, c(1:6)])

 df.1$days<-as.numeric(df.1$days)
 sapply(list(df.1, df.2, df.3, df.4, df.5), dim) ##there are 393 tweeter account
 
 df.6<-Reduce(merge, list(df.1, df.2, df.3, df.4, df.5))  ##the last recode is invalid which does not have the tweeter user-id. Thus, we exclude the last record
 df<-df.6[-393, -2] ##there are 392 user-id with valid data.

 dim(df)
 names(df)
 
 ##the outcome variable is the follower count 
 ##the proportion of number of times mentioning Trump. Since the followers count, the statuses count, the favourites count and the friend count are very larger, these variables are rescaled by dividing them by 100 to put them in reasonable units. 
 df$statuses_count.1<-df$statuses_count/100
 df$favourites_count.1<-df$favourites_count/100
 df$friends_count.1<-df$friends_count/100
 ##trump1 is the proportion of the number of mentioning Trump 
 df$Trump.1<-df$number_trump/df$number_tweets
 df$Clinton.1<-df$number_clinton/df$number_tweets
 names(df)
 
 ##using the proportion of tweets that reference Clinton as the predictor
 ##the length of days should be treated as the exposure
 
glm1<-glm(df$followers_count~df$Clinton.1, offset=log(df$days),family = poisson)
summary(glm1)

stan.glm1<-stan_glm(df$followers_count~df$Clinton.1, offset=log(df$days),family = poisson, prior = normal(0, 2.5), prior_intercept = normal(0, 5))
print(stan.glm1, digit=5)


###comparing the point estimates and standard error of glm and stan_glm. They are nearly identical. 
rbind(glm=coef(glm1), stan_glm=coef(stan.glm1))
rbind(glm=summary(glm1)$coefficients[, 2], stan_glm=se(stan.glm1))
```


###3.2 Bounding the Posterior Distribution of the Coefficients
For the Bayesian's perspective, the central credible intervals are using the information of the prior distribution and the problem-specific contextual information, sometimes the credible interval "put forward as more practical concept than the confidence interval" (https://www.statsdirect.com/help/basics/confidence_interval.htm). For the Bayesians the parameters are random and the data are fixed. In other word, the bounds are fixed and the estimated parameter are random. Thus, the credible intervals are "open to subjective moulding of interpretation". 
The confidence intervals, which is more from the frequentist's perspective, mean that with a large number of repeated samples, you are 90% confidence that this interval contains the true population value. They treat their bounds as random and the parameters as fixed. 
In this study, the 90% CI (credible intervals and confidence intervals) are nearly identical.

For all coefficents, including the intercept, both endpoints of these 90% credible intervals are on the same side of zero.
```{r}
##CI for the stan.glm
ci.stan.glm<-round(posterior_interval(stan.glm1, prob=0.9), 5)

##CI for the glm
ci.glm<-round(confint(glm1, level = 0.90), 5)

round(ci.stan.glm-ci.glm, 3)
ci.glm
ci.stan.glm
```


###3.3 Visualizing the Conclusion
The slopes are very small. 
```{r}
library(ggplot2)
draws <- as.data.frame(as.matrix(stan.glm1))
colnames(draws)[1:2] <- c("intercept_1", "slope_1")


ggplot(df, aes(x = Clinton.1, y =followers_count )) + 
  geom_point(size = 1) +
  geom_abline(data = draws, aes(intercept = intercept_1, slope = slope_1), 
              color = "dark red", size = 2, alpha = 0.25) + 
  geom_abline(intercept = coef(stan.glm1)[1], slope = coef(stan.glm1)[2], 
              color = "light blue", size = 1.5) +
  ylim(0, 30000)

```



